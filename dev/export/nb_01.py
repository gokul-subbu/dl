
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/01_nn.ipynb

from export.nb_00 import *
from torch import nn, optim, tensor, Tensor, hub
import torch.nn.functional as F
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler
import gzip, pickle, torch
from tqdm import tnrange, tqdm_notebook

from functools import partial
from typing import Any, Collection, Callable, NewType, List, Union, TypeVar, Optional, Generator, Iterable

from tqdm.autonotebook import tqdm
tqdm.pandas()


mnist_path=Path(r'd:\datasets\data\mnist.pkl.gz')
# mnist_path.ls()

def get_mnist():
    with gzip.open(mnist_path, 'rb') as f:
        ((x_train, y_train),(x_valid, y_valid), _)=pickle.load(f, encoding='latin-1')
    x_train, y_train, x_valid, y_valid= map(tensor, (x_train, y_train, x_valid, y_valid))
    return (x_train, y_train), (x_valid, y_valid)

class Dataset():
    def __init__(self, x, y):
        self.x, self.y=x, y

    def __getitem__(self, i): return self.x[i], self.y[i]
    def __len__(self): return len(self.x)

def collate(b):
    xs,ys = zip(*b)
    return torch.stack(xs),torch.stack(ys)

def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    for epoch in tnrange(epochs, desc='epochs', smoothing=1):
        # Handle batchnorm / dropout
        model.train()
#         print(model.training)
        for xb,yb in tqdm(train_dl,leave=False, desc='train', smoothing=0.5):
            loss = loss_func(model(xb), yb)
            loss.backward()
            opt.step()
            opt.zero_grad()

        model.eval()
#         print(model.training)
        with torch.no_grad():
            tot_loss,tot_acc = 0.,0.
            for xb,yb in tqdm(valid_dl, leave=False, desc='valid', smoothing=0.5):
                pred = model(xb)
                tot_loss += loss_func(pred, yb)
                tot_acc  += accuracy (pred,yb)
        nv = len(valid_dl)
        tqdm.write(f'{epoch}, {tot_loss/nv}, {tot_acc/nv}')
    return tot_loss/nv, tot_acc/nv

class Lambda(nn.Module):
    def __init__(self, func):
        super().__init__()
        self.func=func

    def forward(self, x): return self.func(x)