{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.all import *\n",
    "from torch import nn, optim, tensor, Tensor\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import operator, torch, gzip\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "from numpy import array, ndarray\n",
    "import pandas as pd\n",
    "from typing import Any, Collection, Callable, NewType, List, Union, TypeVar, Optional, Generator, Iterable\n",
    "import itertools\n",
    "from pandas.api.types import is_categorical_dtype,is_numeric_dtype\n",
    "from numpy import array,ndarray\n",
    "from scipy import ndimage\n",
    "from IPython.core.debugger import set_trace\n",
    "import numpy as np\n",
    "try:\n",
    "    from types import WrapperDescriptorType,MethodWrapperType,MethodDescriptorType\n",
    "except ImportError:\n",
    "    WrapperDescriptorType = type(object.__init__)\n",
    "    MethodWrapperType = type(object().__str__)\n",
    "    MethodDescriptorType = type(str.join)\n",
    "from types import BuiltinFunctionType,BuiltinMethodType,MethodType,FunctionType\n",
    "\n",
    "pd.options.display.max_colwidth = 600\n",
    "NoneType = type(None)\n",
    "string_classes = (str,bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_path=Path(r'd:\\git\\dl\\data')\n",
    "mnist_path=data_path.joinpath('mnist.pkl.gz')\n",
    "#Path.ls=lambda x: L(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('d:/git/dl/data/imagenette2-160'),Path('d:/git/dl/data/imagewoof2-160'),Path('d:/git/dl/data/mnist.pkl.gz')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(mnist_path, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _)=pickle.load(f, encoding='latin-1')\n",
    "    \n",
    "x_train, y_train, x_valid, y_valid=map(tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "epochs=2\n",
    "lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "Rank0Tensor = NewType('OneEltTensor', Tensor)\n",
    "LossFunction = Callable[[Tensor, Tensor], Rank0Tensor]\n",
    "Model = nn.Module\n",
    "\n",
    "def is_listy(x:Any)->bool: return isinstance(x, (tuple,list))\n",
    "\n",
    "def loss_batch(model:Model, xb:Tensor, yb:Tensor, \n",
    "               loss_fn:LossFunction, opt:optim.Optimizer=None):\n",
    "    \"Calculate loss for the batch `xb,yb` and backprop with `opt`\"\n",
    "    if not is_listy(xb): xb = [xb]\n",
    "    if not is_listy(yb): yb = [yb]\n",
    "    loss = loss_fn(model(*xb), *yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fit(epochs:int, model:Model, loss_fn:LossFunction, \n",
    "        opt:optim.Optimizer, train_dl:DataLoader, valid_dl:DataLoader):\n",
    "    \"Train `model` on `train_dl` with `optim` then validate against `valid_dl`\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl: loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "LambdaFunc = Callable[[Tensor],Tensor]\n",
    "class Lambda(nn.Module):\n",
    "    \"An easy way to create a pytorch layer for a simple `func`\"\n",
    "    def __init__(self, func:LambdaFunc):\n",
    "        \"create a layer that simply calls `func` with `x`\"\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x): return x\n",
    "\n",
    "def ResizeBatch(*size:int) -> Tensor: \n",
    "    \"Layer that resizes x to `size`, good for connecting mismatched layers\"\n",
    "    return Lambda(lambda x: x.view((-1,)+size))\n",
    "def Flatten()->Tensor: \n",
    "    \"Flattens `x` to a single dimension, often used at the end of a model\"\n",
    "    return Lambda(lambda x: x.view((x.size(0), -1)))\n",
    "def PoolFlatten()->nn.Sequential:\n",
    "    \"Apply `nn.AdaptiveAvgPool2d` to `x` and then flatten the result\"\n",
    "    return nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten())\n",
    "\n",
    "def conv2d(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias=False) -> nn.Conv2d:\n",
    "    \"Create `nn.Conv2d` layer: `ni` inputs, `nf` outputs, `ks` kernel size. `padding` defaults to `k//2`\"\n",
    "    if padding is None: padding = ks//2\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "def conv2d_relu(ni:int, nf:int, ks:int=3, stride:int=1, \n",
    "                padding:int=None, bn:bool=False) -> nn.Sequential:\n",
    "    \"Create a `conv2d` layer with `nn.ReLU` activation and optional(`bn`) `nn.BatchNorm2d`\"\n",
    "    layers = [conv2d(ni, nf, ks=ks, stride=stride, padding=padding), nn.ReLU()]\n",
    "    if bn: layers.append(nn.BatchNorm2d(nf))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv2d_trans(ni:int, nf:int, ks:int=2, stride:int=2, padding:int=0) -> nn.ConvTranspose2d:\n",
    "    \"Create `nn.nn.ConvTranspose2d` layer: `ni` inputs, `nf` outputs, `ks` kernel size. `padding` defaults to 0\"\n",
    "    return nn.ConvTranspose2d(ni, nf, kernel_size=ks, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist2img(b): return b.view(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class DatasetTfm(Dataset):\n",
    "    \"Applies `tfm` to `ds`\"\n",
    "    ds: Dataset\n",
    "    tfm: Callable = None\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,idx:int):\n",
    "        \"Apply `tfm` to `x` and return `(x[idx],y[idx])`\"\n",
    "        x,y = self.ds[idx]\n",
    "        if self.tfm is not None: x = self.tfm(x)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def simple_cnn(actns:Collection[int], kernel_szs:Collection[int], \n",
    "               strides:Collection[int]) -> nn.Sequential:\n",
    "    \"CNN with `conv2d_relu` layers defined by `actns`, `kernel_szs` and `strides`\"\n",
    "    layers = [conv2d_relu(actns[i], actns[i+1], kernel_szs[i], stride=strides[i])\n",
    "        for i in range(len(strides))]\n",
    "    layers.append(PoolFlatten())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ifnone(a:bool,b:Any):\n",
    "    \"`a` if its not None, otherwise `b`\"\n",
    "    return b if a is None else a\n",
    "\n",
    "default_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "Tensors = Union[Tensor, Collection['Tensors']]\n",
    "\n",
    "def to_device(b:Tensors, device:torch.device):\n",
    "    \"Ensure `b` is on `device`\"\n",
    "    device = ifnone(device, default_device)\n",
    "    if is_listy(b): return [to_device(o, device) for o in b]\n",
    "    return b.to(device)\n",
    "\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    \"`DataLoader` that ensures batches from `dl` are on `device`\"\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "\n",
    "    def __len__(self) -> int: return len(self.dl)\n",
    "    def proc_batch(self,b:Tensors): return to_device(b, self.device)\n",
    "\n",
    "    def __iter__(self)->Tensors:\n",
    "        \"Ensure batches from `dl` are on `device` as we iterate\"\n",
    "        self.gen = map(self.proc_batch, self.dl)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, device:torch.device=default_device, **kwargs): return cls(DataLoader(*args, **kwargs), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "TItem = TypeVar('TItem')\n",
    "TfmCallable = Callable[[TItem],TItem]\n",
    "TfmList = Union[TfmCallable, Collection[TfmCallable]]\n",
    "Tfms = Optional[TfmList]\n",
    "\n",
    "@dataclass\n",
    "class DataBunch():\n",
    "    \"Bind `train_dl`, `valid_dl` to `device`\"\n",
    "    train_dl:DataLoader\n",
    "    valid_dl:DataLoader\n",
    "    device:torch.device=None\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds:Dataset, valid_ds:Dataset, bs:int=64, \n",
    "               train_tfm:Tfms=None, valid_tfm:Tfms=None, device:torch.device=None, **kwargs):\n",
    "        return cls(DeviceDataLoader.create(DatasetTfm(train_ds, train_tfm), bs,   shuffle=True,  device=device, **kwargs),\n",
    "                   DeviceDataLoader.create(DatasetTfm(valid_ds, valid_tfm), bs*2, shuffle=False, device=device, **kwargs),\n",
    "                   device=device)\n",
    "\n",
    "class Learner():\n",
    "    \"Train `model` on `data` for `epochs` using learning rate `lr` and `opt_fn` to optimize training\"\n",
    "    def __init__(self, data:DataBunch, model:Model):\n",
    "        self.data,self.model = data,to_device(model, data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        loss_fn = F.cross_entropy\n",
    "        fit(epochs, self.model, loss_fn, opt, self.data.train_dl, self.data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=mnist2img, valid_tfm=mnist2img)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learner = Learner(data, model)\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.47973348634152474\n"
     ]
    }
   ],
   "source": [
    "learner.fit(1, lr/5, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.26454153081661536\n",
      "1 0.2459865140764019\n",
      "0 0.14068308001077628\n"
     ]
    }
   ],
   "source": [
    "learner.fit(2, lr, opt_fn=opt_fn)\n",
    "learner.fit(1, lr/5, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch env",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
